{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impoting all ther required libraries and modules\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all the relevent features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NzpLqt_cTMPr"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"Consumer_Train.csv\")\n",
    "test_data = pd.read_csv(\"Consumer_Test.csv\")\n",
    "\n",
    "train_data.drop(['Date received','Sub-product','Company public response','Company','State','ZIP code','Tags','Consumer consent provided?','Submitted via','Company response to consumer','Timely response?','Complaint ID','Consumer disputed?','Date sent to company'],axis = 1,inplace =True)\n",
    "test_data.drop(['Date received','Sub-product','Company public response','Company','State','ZIP code','Tags','Consumer consent provided?','Submitted via','Company response to consumer','Timely response?','Complaint ID','Consumer disputed?','Date sent to company'],axis = 1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xWENckywX92k",
    "outputId": "af7f4173-3065-4b3b-bc37-54efa55118cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product                            0\n",
       "Issue                              0\n",
       "Sub-issue                       1996\n",
       "Consumer complaint narrative       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the y_labels to a numerical datatype and transforming the features to vectors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XoqWjZCRbAlZ",
    "outputId": "ba497053-6054-4492-b831-9c6e0165605f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhijit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhijit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#deleting all the null values\n",
    "train_data.dropna(inplace =True)\n",
    "test_data.dropna(inplace =True)\n",
    "\n",
    "text_to_num = {}\n",
    "def convert_to_num(val):\n",
    "    return text_to_num[val]\n",
    "\n",
    "uniq_elements = set(train_data['Product'])\n",
    "x=1\n",
    "for uniq_value in uniq_elements:\n",
    "    if uniq_value not in text_to_num  and uniq_value != -1:\n",
    "        text_to_num[uniq_value] = x\n",
    "        x+=1\n",
    "\n",
    "y_train = list(map(convert_to_num, train_data['Product']))\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "Consumer_complaint_narrative_train = pd.Series(list(train_data['Consumer complaint narrative']),index=list(range(len(train_data))))\n",
    "Consumer_complaint_narrative_test = pd.Series(list(test_data['Consumer complaint narrative']),index=list(range(len(test_data))))\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(Consumer_complaint_narrative_train,y_train, test_size = 0.30,shuffle = True ,random_state = 1234)\n",
    "\n",
    "class preprocessor:\n",
    "    def __init__(self, sentences):\n",
    "        self.sen = sentences\n",
    "\n",
    "    def sent_tokenizer(self,sent):\n",
    "\n",
    "        sent = nltk.sent_tokenize(sent)\n",
    "\n",
    "        for i in range(len(sent)):\n",
    "\n",
    "            sent[i] = sent[i][:len(sent[i])-1]\n",
    "        sent = ' '.join(sent)\n",
    "\n",
    "        return sent\n",
    "\n",
    "    def tokenize_it(self, sen):\n",
    "        # instantiate tokenizer class\n",
    "        sen_new = nltk.word_tokenize(sen)\n",
    "        return sen_new\n",
    "\n",
    "    def remove_stopW_and_punc(self, sen_tokens):\n",
    "        stopwords_english = stopwords.words('english')\n",
    "        sen_clean = []\n",
    "\n",
    "        for word in sen_tokens: # Go through every word in your tokens list\n",
    "            if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "                sen_clean.append(word)\n",
    "        return sen_clean\n",
    "\n",
    "\n",
    "    def sen_stemmer(self, sen_clean):\n",
    "        # Instantiate stemming class\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        # Create an empty list to store the stems\n",
    "        sen_stem = []\n",
    "\n",
    "        for word in sen_clean:\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            sen_stem.append(stem_word)  # append to the list\n",
    "\n",
    "        return sen_stem\n",
    "\n",
    "      # Now we setup a controller which initiates each mehtod of the above declared\n",
    "      # class in the correct chronological order.\n",
    "\n",
    "    def controller(self, sen):\n",
    "        temp_sen = self.sent_tokenizer(sen)\n",
    "        temp_sen = self.tokenize_it(temp_sen)\n",
    "        temp_sen = self.remove_stopW_and_punc(temp_sen)\n",
    "        temp_sen = self.sen_stemmer(temp_sen)\n",
    "\n",
    "        return temp_sen\n",
    "\n",
    "    def run(self):\n",
    "        processed_sens = []\n",
    "        while self.sen:\n",
    "            sen = self.sen.pop(0)\n",
    "            processed_sen = self.controller(sen)\n",
    "            processed_sens.append(processed_sen)\n",
    "\n",
    "        for i in range(len(processed_sens)):\n",
    "            processed_sens[i] = ' '.join(processed_sens[i])\n",
    "\n",
    "        return processed_sens\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "processed_sentences_train = preprocessor(list(np.array(X_train))).run()\n",
    "# vectorization\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "Vectorizer = TfidfVectorizer(use_idf = True)\n",
    "Consumer_complaint_narrative_train = Vectorizer.fit_transform(processed_sentences_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the test dataset to vectors\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F0Aefd5-JHhX"
   },
   "outputs": [],
   "source": [
    "text_to_num = {}\n",
    "def convert_to_num(val):\n",
    "    return text_to_num[val]\n",
    "\n",
    "uniq_elements = set(test_data['Product'])\n",
    "x=1\n",
    "for uniq_value in uniq_elements:\n",
    "    if uniq_value not in text_to_num  and uniq_value != -1:\n",
    "        text_to_num[uniq_value] = x\n",
    "        x+=1\n",
    "\n",
    "y_test = list(map(convert_to_num, test_data['Product']))\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "processed_sentences_test = preprocessor(list(np.array(Consumer_complaint_narrative_test))).run()\n",
    "Consumer_complaint_narrative_test = Vectorizer.transform(processed_sentences_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the logistic regressor model class, fitting it on the training the data and predicting on the test data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(Consumer_complaint_narrative_train.toarray().tolist(),y_train)\n",
    "\n",
    "y_pred_LR = model.predict(Consumer_complaint_narrative_test.toarray().tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauging the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of the Logistic model is: 0.8928997297684136\n"
     ]
    }
   ],
   "source": [
    "print(\"The F1 score of the Logistic model is:\", f1_score(y_test,y_pred_LR,average='weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The f1_score is 89% as I didn't have the required RAM to train whole of the data. I have only used 70% of the data to train which is why the f1_score is not that good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
